{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Approaches to Multi-Label Classification: Motivation\n",
    "\n",
    "### Random Training Dataset\n",
    "\n",
    "1. This is a version where the labels of the training dataset are randomly set, in an attempt to show that better classifiers still perform better even for noisy (high-inconsistently labeled) dataset, but not for randomly assigned dataset.\n",
    "2. This program is modified from https://github.com/nkartik94/Multi-Label-Text-Classification on 2019/11/18 by Yuen-Hsien Tseng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EDA: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "#printmd('**bold**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path  = \"mlabel_corpora/JokeMotivation.txt\"\n",
    "train_path = \"mlabel_corpora/JokeMotivation_train_random.txt\"\n",
    "test_path  = \"mlabel_corpora/JokeMotivation_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3365, 9) (1691, 9) (1674, 9)\n"
     ]
    }
   ],
   "source": [
    "# set global variables: df\n",
    "df = pd.read_csv(data_path, delimiter=\"\\t\")\n",
    "train_random = pd.read_csv(train_path, delimiter=\"\\t\")\n",
    "test = pd.read_csv(test_path, delimiter=\"\\t\")\n",
    "print(df.shape, train_random.shape, test.shape) # same as data_raw.shape in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                  0\n",
      "Title               0\n",
      "Content             0\n",
      "Affinity            0\n",
      "Self_Improvement    0\n",
      "Attack              0\n",
      "Self_Depression     0\n",
      "Taboo               0\n",
      "Others              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values_check = df.isnull().sum()\n",
    "print(missing_values_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Calculating number of jokes under each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jokes with no label are considered to be clean jokes.\n",
    "# Creating seperate column in dataframe to identify clean jokes.\n",
    "# We use axis=1 to count row-wise and axis=0 to count column wise\n",
    "def print_empty_label(df, s):\n",
    "    rowSums = df.iloc[:,3:].sum(axis=1)\n",
    "    #print(rowSums.shape)\n",
    "    #print(rowSums.head())\n",
    "    clean_comments_count = (rowSums==0).sum(axis=0)\n",
    "\n",
    "    print(f\"Total number of {s} jokes = \",len(df))\n",
    "    print(f\"Number of clean jokes in {s}= \",clean_comments_count)\n",
    "    print(f\"Number of {s} jokes with labels =\",(len(df)-clean_comments_count))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of all jokes =  3365\n",
      "Number of clean jokes in all=  6\n",
      "Number of all jokes with labels = 3359\n",
      "\n",
      "Total number of train_random jokes =  1691\n",
      "Number of clean jokes in train_random=  5\n",
      "Number of train_random jokes with labels = 1686\n",
      "\n",
      "Total number of test jokes =  1674\n",
      "Number of clean jokes in test=  1\n",
      "Number of test jokes with labels = 1673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_empty_label(df, 'all')\n",
    "print_empty_label(train_random, 'train_random')\n",
    "print_empty_label(test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Title', 'Content', 'Affinity', 'Self_Improvement', 'Attack', 'Self_Depression', 'Taboo', 'Others']\n",
      "['Affinity', 'Self_Improvement', 'Attack', 'Self_Depression', 'Taboo', 'Others']\n"
     ]
    }
   ],
   "source": [
    "# set global variables: categories\n",
    "categories = list(df.columns.values)\n",
    "print(categories)\n",
    "categories = categories[3:]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating number of jokes in each category\n",
    "def print_category_count(df, categories):\n",
    "    counts = []\n",
    "    for category in categories:\n",
    "        counts.append((category, df[category].sum()))\n",
    "    df_stats = pd.DataFrame(counts, columns=['category', 'number of jokes'])\n",
    "    print(df_stats)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           category  number of jokes\n",
      "0          Affinity               92\n",
      "1  Self_Improvement               43\n",
      "2            Attack              409\n",
      "3   Self_Depression               64\n",
      "4             Taboo              584\n",
      "5            Others             2226\n",
      "\n",
      "           category  number of jokes\n",
      "0          Affinity               73\n",
      "1  Self_Improvement               27\n",
      "2            Attack              266\n",
      "3   Self_Depression               47\n",
      "4             Taboo              355\n",
      "5            Others              972\n",
      "\n",
      "           category  number of jokes\n",
      "0          Affinity               19\n",
      "1  Self_Improvement               16\n",
      "2            Attack              143\n",
      "3   Self_Depression               17\n",
      "4             Taboo              229\n",
      "5            Others             1254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_category_count(df, categories)\n",
    "print_category_count(train_random, categories)\n",
    "print_category_count(test, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_category_count(df, categories):\n",
    "    sns.set(font_scale = 2)\n",
    "    plt.figure(figsize=(15,8))\n",
    "\n",
    "    ax= sns.barplot(categories, df.iloc[:,3:].sum().values)\n",
    "\n",
    "    plt.title(\"Jokes in each category\", fontsize=24)\n",
    "    plt.ylabel('Number of jokes', fontsize=18)\n",
    "    plt.xlabel('Joke Skill', fontsize=18)\n",
    "\n",
    "    #adding the text labels\n",
    "    rects = ax.patches\n",
    "    #print(rects)\n",
    "    labels = df.iloc[:,3:].sum().values\n",
    "    #print(labels)\n",
    "    for rect, label in zip(rects, labels):\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_category_count(df, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Calculating number of jokes having multiple labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_label(mlc_labels, multiLabel_counts):\n",
    "    sns.set(font_scale = 2)\n",
    "    plt.figure(figsize=(15,8))\n",
    "\n",
    "    ax = sns.barplot(mlc_labels, multiLabel_counts.values)\n",
    "\n",
    "    plt.title(\"Jokes having multiple labels \")\n",
    "    plt.ylabel('Number of jokes', fontsize=18)\n",
    "    plt.xlabel('Number of labels', fontsize=18)\n",
    "\n",
    "    #adding the text labels\n",
    "    rects = ax.patches\n",
    "    labels = multiLabel_counts.values\n",
    "    for rect, label in zip(rects, labels):\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_multiple_label(df):\n",
    "    rowSums = df.iloc[:,3:].sum(axis=1)\n",
    "    multiLabel_counts = rowSums.value_counts()\n",
    "    print(multiLabel_counts)\n",
    "    multiLabel_counts = multiLabel_counts.iloc[:]\n",
    "    #print(multiLabel_counts.index)\n",
    "    mlc_labels = ['L'+str(i) for i in multiLabel_counts.index]\n",
    "    print(mlc_labels)\n",
    "    \n",
    "    #plot_multiple_label(mlc_labels, multiLabel_counts)\n",
    "    ##return(mlc_labels, multiLabel_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    3301\n",
      "2      57\n",
      "0       6\n",
      "3       1\n",
      "dtype: int64\n",
      "['L1', 'L2', 'L0', 'L3']\n"
     ]
    }
   ],
   "source": [
    "print_multiple_label(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. WordCloud representation of most used words in each category of jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import jieba\n",
    "import Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i, c in enumerate(categories):\\n    plt = MyWordCloud(plt, df, c, i+1)\\n\\n#plt.show()\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2880x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(40,25))\n",
    "\n",
    "def MyWordCloud(plt, df, field, position):\n",
    "    #subset = df[df.Pun==1]\n",
    "    subset = df.loc[df[field] == 1] # https://stackoverflow.com/questions/17071871/how-to-select-rows-from-a-dataframe-based-on-column-values\n",
    "    #print(subset.head()); exit\n",
    "    text = str(subset.Content.values)\n",
    "    words_list = jieba.lcut(Stopwords.clean_text(text))\n",
    "    text = Stopwords.clean_words(words_list)\n",
    "    cloud = WordCloud(\n",
    "                          #stopwords=STOPWORDS,\n",
    "                          stopwords=Stopwords.STOP_WORDS,\n",
    "                          background_color='black',\n",
    "                          font_path='SNsanafonGyou.ttf', # OSError: unknown file format\n",
    "                          collocations=False,\n",
    "                          width=2500,\n",
    "                          height=1800\n",
    "                         ).generate(\" \".join(text))\n",
    "\n",
    "    plt.subplot(3, 3, position)\n",
    "    plt.axis('off')\n",
    "    plt.title(field, fontsize=40)\n",
    "    plt.imshow(cloud)\n",
    "    return plt\n",
    "\n",
    "'''\n",
    "for i, c in enumerate(categories):\n",
    "    plt = MyWordCloud(plt, df, c, i+1)\n",
    "\n",
    "#plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in   all jokes: Max=2024, Min=10, Avg=134.07637444279345\n",
      "Number of characters in train jokes: Max=2024, Min=10, Avg=132.7906564163217\n",
      "Number of characters in  test jokes: Max=874, Min=12, Avg=135.3751493428913\n"
     ]
    }
   ],
   "source": [
    "# Compute statistics of the dataset: MaxLength, MinLength, AvgChars, AvgWords\n",
    "Len = df.Content.map(len)\n",
    "print(f'Number of characters in   all jokes: Max={max(Len)}, Min={min(Len)}, Avg={sum(Len)/len(Len)}')\n",
    "Len = train_random.Content.map(len)\n",
    "print(f'Number of characters in train jokes: Max={max(Len)}, Min={min(Len)}, Avg={sum(Len)/len(Len)}')\n",
    "Len = test.Content.map(len)\n",
    "print(f'Number of characters in  test jokes: Max={max(Len)}, Min={min(Len)}, Avg={sum(Len)/len(Len)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3365, 9)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set global variables: data\n",
    "data = df\n",
    "#data = df.loc[np.random.choice(df.index, size=3365)]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Stopwords # import my own module with STOP_WORDS\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "    '''\n",
    "    Given a raw text string, return a clean text string.\n",
    "    Example: \n",
    "        input:  \"Years  passed. 多少   年过 去 了 。  \"\n",
    "        output: \"years passed.多少年过去了。\"\n",
    "    '''\n",
    "    text = str(text)\n",
    "    text = text.lower() # 'years  passed. 多少   年过 去 了 。'\n",
    "    # Next line will remove redundant white space for jeiba to cut\n",
    "    text = re.sub(r'\\s+([^a-zA-Z0-9.])', r'\\1', text) # years passed.多少年过去了。\n",
    "# see: https://stackoverflow.com/questions/16720541/python-string-replace-regular-expression\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "def clean_words(text, RmvStopWord=True, RmvMark=True):\n",
    "    words = jieba.lcut(text)\n",
    "#    print(\"After jieba.lcut():\", words)\n",
    "#    WL = [ w \n",
    "    WL = [ ps.stem(w)\n",
    "#    WL = [ wnl.lemmatize(w)\n",
    "        for w in words \n",
    "          if (not re.match(r'\\s', w)) # remove white spaces\n",
    "            and (RmvMark==False or not re.match(r'\\W', w)) # remove punctuations\n",
    "#            and (RmvMark==False or not re.match('^[a-z_]$', w)) # remove punctuations\n",
    "#            and (RmvMark==False or w not in PUNCTUATIONS)\n",
    "            and (RmvStopWord==False or w not in Stopwords.STOP_WORDS)\n",
    "            and (not re.match(r'^\\d+$', w)) # remove digit\n",
    "         ]\n",
    "    WL = \" \".join(WL)\n",
    "    return WL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/kg/jcdj05xn20144cv9kwywp26r0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID Title                                            Content  Affinity  \\\n",
      "0  L0001  要求加薪  員工：老闆，您必須幫我加薪，已經有三家公司在找我了！     老闆：哪三家？     員工：...         0   \n",
      "1  L0002  查無此人  某市政府辦公大樓落成，門口缺副對聯。     副市長揮毫     上聯：說實話辦實事一身正氣...         0   \n",
      "2  L0003   遣散費  中午老闆視察自己的建築工地時，發現有個人在角落玩手機。     老闆：你月薪多少？     ...         0   \n",
      "3  L0004  職業習慣  一天，一位法官的女友看見兩個蚊子，便叫法官打死。     只見法官只把那個肚子飽飽的蚊子打死...         0   \n",
      "4  L0005  美女吵架  辦公室中兩位女同事吵起來了。     經理忍無可忍：「太不像話了！現在是什麼情況？你們把原因...         0   \n",
      "\n",
      "   Self_Improvement  Attack  Self_Depression  Taboo  Others  \n",
      "0                 0       0                1      0       0  \n",
      "1                 0       1                0      0       0  \n",
      "2                 0       0                1      0       0  \n",
      "3                 0       0                0      0       1  \n",
      "4                 0       1                0      0       0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.617 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "data['Content'] = data['Content'].str.lower()\n",
    "#data['Content'] = data['Content'].apply(cleanHtml)\n",
    "#data['Content'] = data['Content'].apply(cleanPunc)\n",
    "#data['Content'] = data['Content'].apply(keepAlpha)\n",
    "data['Content'] = data['Content'].apply(clean_text)\n",
    "data['Content'] = data['Content'].apply(clean_words)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "\n",
    "data['Content'] = data['Content'].apply(removeStopWords)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Affinity</th>\n",
       "      <th>Self_Improvement</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Self_Depression</th>\n",
       "      <th>Taboo</th>\n",
       "      <th>Others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L0001</td>\n",
       "      <td>要求加薪</td>\n",
       "      <td>員工 老 闆 必須 幫 加薪 已經 三家 公司 找 老 闆 三家 員工 自來 水 公司 台電...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L0002</td>\n",
       "      <td>查無此人</td>\n",
       "      <td>某 市政府 辦公大樓 落成 門口 缺 副 對聯 副 市長 揮 毫上 聯 實話 辦實事 一身 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L0003</td>\n",
       "      <td>遣散費</td>\n",
       "      <td>中午 老 闆 視察 自己 建築 工地 時 發現 個 人 角落 玩手 機 老 闆 月薪 多少 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L0004</td>\n",
       "      <td>職業習慣</td>\n",
       "      <td>一天 一位 法官 女友 看見 兩個 蚊子 便 叫 法官 打死 只見 法官 只 那個 肚子 飽...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L0005</td>\n",
       "      <td>美女吵架</td>\n",
       "      <td>辦 公室 中 兩位 女同事 吵起 經理 忍無可忍 太不像 話 情況 原因 給我 清楚 兩人 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID Title                                            Content  Affinity  \\\n",
       "0  L0001  要求加薪  員工 老 闆 必須 幫 加薪 已經 三家 公司 找 老 闆 三家 員工 自來 水 公司 台電...         0   \n",
       "1  L0002  查無此人  某 市政府 辦公大樓 落成 門口 缺 副 對聯 副 市長 揮 毫上 聯 實話 辦實事 一身 ...         0   \n",
       "2  L0003   遣散費  中午 老 闆 視察 自己 建築 工地 時 發現 個 人 角落 玩手 機 老 闆 月薪 多少 ...         0   \n",
       "3  L0004  職業習慣  一天 一位 法官 女友 看見 兩個 蚊子 便 叫 法官 打死 只見 法官 只 那個 肚子 飽...         0   \n",
       "4  L0005  美女吵架  辦 公室 中 兩位 女同事 吵起 經理 忍無可忍 太不像 話 情況 原因 給我 清楚 兩人 ...         0   \n",
       "\n",
       "   Self_Improvement  Attack  Self_Depression  Taboo  Others  \n",
       "0                 0       0                1      0       0  \n",
       "1                 0       1                0      0       0  \n",
       "2                 0       0                1      0       0  \n",
       "3                 0       0                0      0       1  \n",
       "4                 0       1                0      0       0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "data['Content'] = data['Content'].apply(stemming)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1691, 9)\n",
      "(1674, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set global variables: train, test\n",
    "#train, test = train_test_split(data, random_state=42, test_size=0.30, shuffle=True)\n",
    "train, test = train_test_split(data, random_state=42, train_size=1691, shuffle=False)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables: train_text, test_text\n",
    "train_text = train['Content']\n",
    "test_text = test['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in   all jokes: Max=491, Min=3, Avg=41.6222882615156\n",
      "Number of words in train jokes: Max=491, Min=3, Avg=40.33234772324069\n",
      "Number of words in  test jokes: Max=290, Min=4, Avg=42.92532855436081\n"
     ]
    }
   ],
   "source": [
    "# Compute statistics of the dataset: MaxLength, MinLength, AvgChars, AvgWords\n",
    "Len = data.Content.map(lambda x: len(x.split()))\n",
    "print(f'Number of words in   all jokes: Max={max(Len)}, Min={min(Len)}, Avg={sum(Len)/len(Len)}')\n",
    "Len = train.Content.map(lambda x: len(x.split()))\n",
    "print(f'Number of words in train jokes: Max={max(Len)}, Min={min(Len)}, Avg={sum(Len)/len(Len)}')\n",
    "Len = test.Content.map(lambda x: len(x.split()))\n",
    "print(f'Number of words in  test jokes: Max={max(Len)}, Min={min(Len)}, Avg={sum(Len)/len(Len)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', \n",
    "                             ngram_range=(1,2), norm='l2')\n",
    "vectorizer.fit(train_text)\n",
    "vectorizer.fit(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set global variables: x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer.transform(train_text)\n",
    "y_train = train_random.drop(labels = ['ID', 'Title', 'Content'], axis=1)\n",
    "\n",
    "x_test = vectorizer.transform(test_text)\n",
    "y_test = test.drop(labels = ['ID', 'Title', 'Content'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain_tfidf.shape:(1691, 9559), xtest_tfidf.shape: (1674, 9559)\n",
      "xtrain_tfidf_ngram.shape:(1691, 9559), xtest_tfidf_ngram.shape: (1674, 9559)\n",
      "xtrain_tfidf_ngram_chars.shape:(1691, 9559), xtest_tfidf_ngram_chars.shape: (1674, 9559)\n",
      "It takes 2.49 seconds to convert 3 TFxIDF vectors.\n"
     ]
    }
   ],
   "source": [
    "time_TfidfVector = time.time()\n",
    "\n",
    "def Create_TFxIDF(data_text, train_text, test_text):\n",
    "\n",
    "# word level tf-idf\n",
    "    #tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000)\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "        stop_words=Stopwords.STOP_WORDS, max_df=0.95, min_df=2, max_features=10000)\n",
    "    tfidf_vect.fit(data_text)\n",
    "    xtrain_tfidf = tfidf_vect.transform(train_text)\n",
    "    xtest_tfidf = tfidf_vect.transform(test_text)\n",
    "    print(f\"xtrain_tfidf.shape:{xtrain_tfidf.shape}, xtest_tfidf.shape: {xtest_tfidf.shape}\")\n",
    "\n",
    "# word level ngram tf-idf \n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                    stop_words=Stopwords.STOP_WORDS, max_df=0.95, min_df=2,\n",
    "                    ngram_range=(2,3), max_features=10000)\n",
    "    tfidf_vect_ngram.fit(data_text)\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_text)\n",
    "    xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_text)\n",
    "    print(f\"xtrain_tfidf_ngram.shape:{xtrain_tfidf.shape}, xtest_tfidf_ngram.shape: {xtest_tfidf.shape}\")\n",
    "\n",
    "# character level ngram tf-idf\n",
    "    tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', \n",
    "                    stop_words=Stopwords.STOP_WORDS, max_df=0.95, min_df=2,\n",
    "                    ngram_range=(2,3), max_features=10000)\n",
    "    tfidf_vect_ngram_chars.fit(data_text)\n",
    "    xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_text) \n",
    "    xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_text) \n",
    "    print(f\"xtrain_tfidf_ngram_chars.shape:{xtrain_tfidf.shape}, xtest_tfidf_ngram_chars.shape: {xtest_tfidf.shape}\")\n",
    "\n",
    "    print(\"It takes %4.2f seconds to convert 3 TFxIDF vectors.\"%(time.time()-time_TfidfVector))\n",
    "\n",
    "    return (xtrain_tfidf, xtest_tfidf, \n",
    "             xtrain_tfidf_ngram, xtest_tfidf_ngram,\n",
    "             xtrain_tfidf_ngram_chars, xtest_tfidf_ngram_chars,\n",
    "            tfidf_vect, tfidf_vect_ngram, tfidf_vect_ngram_chars)\n",
    "\n",
    "(xtrain_tfidf, xtest_tfidf, \n",
    " xtrain_tfidf_ngram, xtest_tfidf_ngram,\n",
    " xtrain_tfidf_ngram_chars, xtest_tfidf_ngram_chars,\n",
    " tfidf_vect, tfidf_vect_ngram, tfidf_vect_ngram_chars) = Create_TFxIDF(data.Content, train_text, test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1691, 9559) (1674, 9559)\n"
     ]
    }
   ],
   "source": [
    "# re-assign x_train and x_test to what we want\n",
    "x_train, x_test, vectorizer = xtrain_tfidf, xtest_tfidf, tfidf_vect\n",
    "#x_train, x_test, vectorizer = xtrain_tfidf_ngram, xtest_tfidf_ngram, tfidf_vect_ngram\n",
    "#x_train, x_test, vectorizer = xtrain_tfidf_ngram_chars, xtest_tfidf_ngram_chars, tfidf_vect_ngram_chars\n",
    "print(x_train.shape, x_test.shape)\n",
    "#print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Multiple Binary Classifications - (One Vs Rest Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the evaluation metrics\n",
    "def tcfunc(x, n=4): # trancate a number to have n decimal digits\n",
    "    d = '0' * n\n",
    "    d = int('1' + d)\n",
    "# https://stackoverflow.com/questions/4541155/check-if-a-number-is-int-or-float\n",
    "    if isinstance(x, (int, float)): return int(x * d) / d\n",
    "    return x\n",
    "\n",
    "def print_cls_report(y_true, prediction):\n",
    "    print('Test accuracy is %1.4f'%(accuracy_score(y_true, prediction)))\n",
    "\n",
    "    print(classification_report(y_true, prediction))\n",
    "    \n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
    "    print(\"\\tPrecision\\tRecall\\tF1\\tSupport\")\n",
    "    (Precision, Recall, F1, Support) = list(map(tcfunc, \n",
    "        precision_recall_fscore_support(y_true, prediction, average='micro')))\n",
    "    print(\"Micro\\t{}\\t{}\\t{}\\t{}\".format(Precision, Recall, F1, Support))\n",
    "\n",
    "    (Precision, Recall, F1, Support) = list(map(tcfunc, \n",
    "        precision_recall_fscore_support(y_true, prediction, average='macro')))\n",
    "    print(\"Macro\\t{}\\t{}\\t{}\\t{}\".format(Precision, Recall, F1, Support))\n",
    "    \n",
    "#    if True:\n",
    "    if False:\n",
    "        print(confusion_matrix(y_true, prediction))\n",
    "        try: \n",
    "            print(classification_report(y_true, prediction, digits=4))\n",
    "        except ValueError:\n",
    "            print('May be some category has no predicted samples')\n",
    "        show_confusion_matrix(prediction)\n",
    "\n",
    "\n",
    "    print(f'y_true.shape={y_true.shape}, prediction.shape={prediction.shape}')\n",
    "    #print(y_true.head())\n",
    "    #print(prediction[0:6])\n",
    "\n",
    "    # https://stackoverflow.com/questions/152580/whats-the-canonical-way-to-check-for-type-in-python\n",
    "    pred = prediction\n",
    "    if not isinstance(pred, np.ndarray): pred = prediction.toarray()\n",
    "    print(type(y_true), type(prediction), type(pred))\n",
    "    try:\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "        print('macro roc_auc_score is %1.4f'%(roc_auc_score(y_true, pred, average='macro'))) # default average=’macro’\n",
    "        print('micro roc_auc_score is %1.4f'%(roc_auc_score(y_true, pred, average='micro')))\n",
    "    except:\n",
    "        print(\"roc_auc_score error!!!\")\n",
    "    try:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, pred)\n",
    "        print(f'fpr={fpr}\\ntpr={tpr}\\nthresholds={thresholds}')\n",
    "    except:\n",
    "        print('roc_curve error!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Processing Affinity jokes...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9886\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1655\n",
      "           1       0.00      0.00      0.00        19\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1674\n",
      "   macro avg       0.49      0.50      0.50      1674\n",
      "weighted avg       0.98      0.99      0.98      1674\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.9886\t0.9886\t0.9886\tNone\n",
      "Macro\t0.4943\t0.5\t0.4971\tNone\n",
      "y_true.shape=(1674,), prediction.shape=(1674,)\n",
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5000\n",
      "micro roc_auc_score is 0.5000\n",
      "fpr=[0. 1.]\n",
      "tpr=[0. 1.]\n",
      "thresholds=[1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Self_Improvement jokes...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      1658\n",
      "           1       0.00      0.00      0.00        16\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1674\n",
      "   macro avg       0.50      0.50      0.50      1674\n",
      "weighted avg       0.98      0.99      0.99      1674\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.9904\t0.9904\t0.9904\tNone\n",
      "Macro\t0.4952\t0.5\t0.4975\tNone\n",
      "y_true.shape=(1674,), prediction.shape=(1674,)\n",
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5000\n",
      "micro roc_auc_score is 0.5000\n",
      "fpr=[0. 1.]\n",
      "tpr=[0. 1.]\n",
      "thresholds=[1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Attack jokes...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.96      1531\n",
      "           1       0.00      0.00      0.00       143\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      1674\n",
      "   macro avg       0.46      0.50      0.48      1674\n",
      "weighted avg       0.84      0.91      0.87      1674\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.9145\t0.9145\t0.9145\tNone\n",
      "Macro\t0.4572\t0.5\t0.4776\tNone\n",
      "y_true.shape=(1674,), prediction.shape=(1674,)\n",
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5000\n",
      "micro roc_auc_score is 0.5000\n",
      "fpr=[0. 1.]\n",
      "tpr=[0. 1.]\n",
      "thresholds=[1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Self_Depression jokes...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9898\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1657\n",
      "           1       0.00      0.00      0.00        17\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      1674\n",
      "   macro avg       0.49      0.50      0.50      1674\n",
      "weighted avg       0.98      0.99      0.98      1674\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.9898\t0.9898\t0.9898\tNone\n",
      "Macro\t0.4949\t0.5\t0.4974\tNone\n",
      "y_true.shape=(1674,), prediction.shape=(1674,)\n",
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5000\n",
      "micro roc_auc_score is 0.5000\n",
      "fpr=[0. 1.]\n",
      "tpr=[0. 1.]\n",
      "thresholds=[1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Taboo jokes...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.8632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93      1445\n",
      "           1       0.00      0.00      0.00       229\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      1674\n",
      "   macro avg       0.43      0.50      0.46      1674\n",
      "weighted avg       0.75      0.86      0.80      1674\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.8632\t0.8632\t0.8632\tNone\n",
      "Macro\t0.4316\t0.5\t0.4632\tNone\n",
      "y_true.shape=(1674,), prediction.shape=(1674,)\n",
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5000\n",
      "micro roc_auc_score is 0.5000\n",
      "fpr=[0. 1.]\n",
      "tpr=[0. 1.]\n",
      "thresholds=[1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Others jokes...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.7037\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.08      0.12       420\n",
      "           1       0.75      0.91      0.82      1254\n",
      "\n",
      "   micro avg       0.70      0.70      0.70      1674\n",
      "   macro avg       0.49      0.50      0.47      1674\n",
      "weighted avg       0.62      0.70      0.65      1674\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.7037\t0.7037\t0.7037\tNone\n",
      "Macro\t0.4898\t0.4958\t0.4697\tNone\n",
      "y_true.shape=(1674,), prediction.shape=(1674,)\n",
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.4958\n",
      "micro roc_auc_score is 0.4958\n",
      "fpr=[0.         0.92142857 1.        ]\n",
      "tpr=[0.         0.91307815 1.        ]\n",
      "thresholds=[2 1 0]\n",
      "CPU times: user 105 ms, sys: 78.3 ms, total: 183 ms\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "\n",
    "#for category in categories:\n",
    "for category in categories:\n",
    "    printmd('**Processing {} jokes...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(x_train, train_random[category])\n",
    "    \n",
    "    prediction = LogReg_pipeline.predict(x_test)\n",
    "    \n",
    "    print_cls_report(test[category], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Multiple Binary Classifications - (Binary Relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use binary relevance, run \"pip install scikit-multilearn\" in advance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Next line refers to: http://scikit.ml/tutorial.html\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**BinaryRelevance with GaussianNB()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.4815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        19\n",
      "           1       0.00      0.00      0.00        16\n",
      "           2       0.05      0.03      0.04       143\n",
      "           3       0.00      0.00      0.00        17\n",
      "           4       0.11      0.07      0.09       229\n",
      "           5       0.74      0.65      0.69      1254\n",
      "\n",
      "   micro avg       0.62      0.50      0.55      1678\n",
      "   macro avg       0.15      0.13      0.14      1678\n",
      "weighted avg       0.57      0.50      0.53      1678\n",
      " samples avg       0.49      0.50      0.49      1678\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.6156\t0.4982\t0.5507\tNone\n",
      "Macro\t0.1508\t0.1263\t0.1371\tNone\n",
      "y_true.shape=(1674, 6), prediction.shape=(1674, 6)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'scipy.sparse.csc.csc_matrix'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.4934\n",
      "micro roc_auc_score is 0.7179\n",
      "roc_curve error!!!\n",
      "CPU times: user 10.3 s, sys: 780 ms, total: 11.1 s\n",
      "Wall time: 3.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initialize binary relevance multi-label classifier\n",
    "#   with a gaussian naive bayes base classifier\n",
    "classifier = BinaryRelevance(GaussianNB())\n",
    "\n",
    "# train\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "printmd('**BinaryRelevance with GaussianNB()**')\n",
    "print_cls_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**BinaryRelevance with LinearSVC()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.4277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        19\n",
      "           1       0.00      0.00      0.00        16\n",
      "           2       0.02      0.01      0.02       143\n",
      "           3       0.00      0.00      0.00        17\n",
      "           4       0.10      0.07      0.08       229\n",
      "           5       0.74      0.61      0.67      1254\n",
      "\n",
      "   micro avg       0.59      0.46      0.52      1678\n",
      "   macro avg       0.14      0.12      0.13      1678\n",
      "weighted avg       0.57      0.46      0.51      1678\n",
      " samples avg       0.45      0.47      0.45      1678\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.5905\t0.4642\t0.5198\tNone\n",
      "Macro\t0.1425\t0.1157\t0.1277\tNone\n",
      "y_true.shape=(1674, 6), prediction.shape=(1674, 6)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'scipy.sparse.csc.csc_matrix'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.4891\n",
      "micro roc_auc_score is 0.6998\n",
      "roc_curve error!!!\n",
      "CPU times: user 1.27 s, sys: 175 ms, total: 1.44 s\n",
      "Wall time: 652 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Next line refers to: http://scikit.ml/tutorial.html and \n",
    "#   http://scikit.ml/api/skmultilearn.problem_transform.br.html\n",
    "#classifier = BinaryRelevance(classifier=SVC(), require_dense=[False, True]) # 0.5 very bad!\n",
    "# https://scikit-learn.org/stable/modules/svm.html#unbalanced-problems\n",
    "classifier = BinaryRelevance(classifier=LinearSVC(class_weight='balanced')) # 0.5314\n",
    "#classifier = BinaryRelevance(classifier=LinearSVC()) # Test roc_auc_score is 0.5234\n",
    "\n",
    "# train\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "printmd('**BinaryRelevance with LinearSVC()**')\n",
    "print_cls_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Classifier Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using classifier chains\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Classifier Chains with LinearSVM()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.5597\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        19\n",
      "           1       0.00      0.00      0.00        16\n",
      "           2       0.03      0.02      0.02       143\n",
      "           3       0.00      0.00      0.00        17\n",
      "           4       0.10      0.14      0.12       229\n",
      "           5       0.73      0.72      0.73      1254\n",
      "\n",
      "   micro avg       0.56      0.56      0.56      1678\n",
      "   macro avg       0.14      0.15      0.14      1678\n",
      "weighted avg       0.57      0.56      0.56      1678\n",
      " samples avg       0.56      0.56      0.56      1678\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.5609\t0.5595\t0.5602\tNone\n",
      "Macro\t0.1438\t0.1469\t0.1448\tNone\n",
      "y_true.shape=(1674, 6), prediction.shape=(1674, 6)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'scipy.sparse.csc.csc_matrix'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.4855\n",
      "micro roc_auc_score is 0.7359\n",
      "roc_curve error!!!\n",
      "CPU times: user 2.43 s, sys: 738 ms, total: 3.17 s\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initialize classifier chains multi-label classifier\n",
    "#classifier = ClassifierChain(LogisticRegression()) # Test roc_auc_score is 0.5159\n",
    "classifier = ClassifierChain(LinearSVC(class_weight='balanced')) #  0.5327\n",
    "\n",
    "# Training logistic regression model on train data\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "printmd('**Classifier Chains with LinearSVM()**')\n",
    "print_cls_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Label Powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Label Powerset\n",
    "from skmultilearn.problem_transform import LabelPowerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Label Powerset with LinearSVM()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.5233\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        19\n",
      "           1       0.00      0.00      0.00        16\n",
      "           2       0.04      0.04      0.04       143\n",
      "           3       0.00      0.00      0.00        17\n",
      "           4       0.09      0.09      0.09       229\n",
      "           5       0.74      0.69      0.71      1254\n",
      "\n",
      "   micro avg       0.53      0.53      0.53      1678\n",
      "   macro avg       0.14      0.14      0.14      1678\n",
      "weighted avg       0.57      0.53      0.55      1678\n",
      " samples avg       0.53      0.53      0.53      1678\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.5257\t0.5297\t0.5277\tNone\n",
      "Macro\t0.1448\t0.1368\t0.1406\tNone\n",
      "y_true.shape=(1674, 6), prediction.shape=(1674, 6)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'scipy.sparse.lil.lil_matrix'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.4813\n",
      "micro roc_auc_score is 0.7170\n",
      "roc_curve error!!!\n",
      "CPU times: user 876 ms, sys: 61.8 ms, total: 938 ms\n",
      "Wall time: 295 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initialize label powerset multi-label classifier\n",
    "#classifier = LabelPowerset(LogisticRegression()) # Test roc_auc_score is 0.5059\n",
    "classifier = LabelPowerset(LinearSVC(class_weight='balanced')) # 0.5541\n",
    "# Test roc_auc_score is 0.5312 if xtrain_tfidf_ngram, xtest_tfidf_ngram are used.\n",
    "# Test roc_auc_score is 0.5474 if xtrain_tfidf_ngram_chars, xtest_tfidf_ngram_chars are used\n",
    "\n",
    "# train\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "printmd('**Label Powerset with LinearSVM()**')\n",
    "print_cls_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out/Skill_True.txt', 'w') as outF:\n",
    "    outF.write(y_test.to_csv(sep='\\t', index=False))\n",
    "\n",
    "# https://stackoverflow.com/questions/36967666/transform-scipy-sparse-csr-to-pandas\n",
    "with open('out/Skill_Pred_random.txt', 'w') as outF:\n",
    "    outF.write(pd.DataFrame(predictions.toarray(), columns=list(y_test.columns)).to_csv(sep='\\t', index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Adapted Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit.ml/api/api/skmultilearn.adapt.html#skmultilearn.adapt.MLkNN\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**MLkNN**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.4719\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        19\n",
      "           1       0.00      0.00      0.00        16\n",
      "           2       0.00      0.00      0.00       143\n",
      "           3       0.00      0.00      0.00        17\n",
      "           4       0.00      0.00      0.00       229\n",
      "           5       0.75      0.63      0.69      1254\n",
      "\n",
      "   micro avg       0.74      0.47      0.58      1678\n",
      "   macro avg       0.12      0.11      0.11      1678\n",
      "weighted avg       0.56      0.47      0.51      1678\n",
      " samples avg       0.47      0.47      0.47      1678\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.742\t0.4713\t0.5765\tNone\n",
      "Macro\t0.1249\t0.1051\t0.1141\tNone\n",
      "y_true.shape=(1674, 6), prediction.shape=(1674, 6)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'scipy.sparse.lil.lil_matrix'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.4996\n",
      "micro roc_auc_score is 0.7193\n",
      "roc_curve error!!!\n",
      "CPU times: user 1min 10s, sys: 323 ms, total: 1min 11s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classifier_new = MLkNN(k=10)\n",
    "\n",
    "# Note that this classifier can throw up errors when handling sparse matrices.\n",
    "\n",
    "x_train = lil_matrix(x_train).toarray()\n",
    "y_train = lil_matrix(y_train).toarray()\n",
    "x_test = lil_matrix(x_test).toarray()\n",
    "\n",
    "# train\n",
    "classifier_new.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions_new = classifier_new.predict(x_test)\n",
    "\n",
    "printmd('**MLkNN**')\n",
    "print_cls_report(y_test, predictions_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
