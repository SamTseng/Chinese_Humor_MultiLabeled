{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Approaches to Multi-Label Classification: \n",
    "\n",
    "### Random Training Dataset\n",
    "\n",
    "1. This is a version where the labels of the training dataset are randomly set, in an attempt to show that better classifiers still perform better even for noisy (high-inconsistently labeled) dataset, but not for randomly assigned dataset.\n",
    "2. This program is modified from https://github.com/nkartik94/Multi-Label-Text-Classification on 2019/11/18 by Yuen-Hsien Tseng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EDA: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "#printmd('**bold**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path  = \"mlabel_corpora/JokeSkill.txt\"\n",
    "train_path = \"mlabel_corpora/JokeSkill_train_random.txt\"\n",
    "test_path  = \"mlabel_corpora/JokeSkill_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3365, 11) (1691, 11) (1674, 11)\n"
     ]
    }
   ],
   "source": [
    "# set global variables: df\n",
    "df = pd.read_csv(data_path, delimiter=\"\\t\")\n",
    "train_random = pd.read_csv(train_path, delimiter=\"\\t\")\n",
    "test = pd.read_csv(test_path, delimiter=\"\\t\")\n",
    "print(df.shape, train_random.shape, test.shape) # same as data_raw.shape in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                  0\n",
      "Title               0\n",
      "Content             0\n",
      "Pun                 0\n",
      "Exaggeration        0\n",
      "Anthropomorphism    0\n",
      "Bridge_Inference    0\n",
      "Illogical           0\n",
      "Irony               0\n",
      "Imitation           0\n",
      "Others              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values_check = df.isnull().sum()\n",
    "print(missing_values_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Calculating number of jokes under each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jokes with no label are considered to be clean jokes.\n",
    "# Creating seperate column in dataframe to identify clean jokes.\n",
    "# We use axis=1 to count row-wise and axis=0 to count column wise\n",
    "def print_empty_label(df, s):\n",
    "    rowSums = df.iloc[:,3:].sum(axis=1)\n",
    "    #print(rowSums.shape)\n",
    "    #print(rowSums.head())\n",
    "    clean_comments_count = (rowSums==0).sum(axis=0)\n",
    "\n",
    "    print(f\"Total number of {s} jokes = \",len(df))\n",
    "    print(f\"Number of clean jokes in {s}= \",clean_comments_count)\n",
    "    print(f\"Number of {s} jokes with labels =\",(len(df)-clean_comments_count))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of all jokes =  3365\n",
      "Number of clean jokes in all=  2\n",
      "Number of all jokes with labels = 3363\n",
      "\n",
      "Total number of train_random jokes =  1691\n",
      "Number of clean jokes in train_random=  2\n",
      "Number of train_random jokes with labels = 1689\n",
      "\n",
      "Total number of test jokes =  1674\n",
      "Number of clean jokes in test=  0\n",
      "Number of test jokes with labels = 1674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_empty_label(df, 'all')\n",
    "print_empty_label(train_random, 'train_random')\n",
    "print_empty_label(test, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Title', 'Content', 'Pun', 'Exaggeration', 'Anthropomorphism', 'Bridge_Inference', 'Illogical', 'Irony', 'Imitation', 'Others']\n",
      "['Pun', 'Exaggeration', 'Anthropomorphism', 'Bridge_Inference', 'Illogical', 'Irony', 'Imitation', 'Others']\n"
     ]
    }
   ],
   "source": [
    "# set global variables: categories\n",
    "categories = list(df.columns.values)\n",
    "print(categories)\n",
    "categories = categories[3:]\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating number of jokes in each category\n",
    "def print_category_count(df, categories):\n",
    "    counts = []\n",
    "    for category in categories:\n",
    "        counts.append((category, df[category].sum()))\n",
    "    df_stats = pd.DataFrame(counts, columns=['category', 'number of jokes'])\n",
    "    print(df_stats)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           category  number of jokes\n",
      "0               Pun             1039\n",
      "1      Exaggeration              131\n",
      "2  Anthropomorphism              180\n",
      "3  Bridge_Inference              607\n",
      "4         Illogical              924\n",
      "5             Irony               82\n",
      "6         Imitation              258\n",
      "7            Others              588\n",
      "\n",
      "           category  number of jokes\n",
      "0               Pun              778\n",
      "1      Exaggeration               71\n",
      "2  Anthropomorphism              140\n",
      "3  Bridge_Inference              316\n",
      "4         Illogical              438\n",
      "5             Irony               47\n",
      "6         Imitation              148\n",
      "7            Others              110\n",
      "\n",
      "           category  number of jokes\n",
      "0               Pun              261\n",
      "1      Exaggeration               60\n",
      "2  Anthropomorphism               40\n",
      "3  Bridge_Inference              291\n",
      "4         Illogical              486\n",
      "5             Irony               35\n",
      "6         Imitation              110\n",
      "7            Others              478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_category_count(df, categories)\n",
    "print_category_count(train_random, categories)\n",
    "print_category_count(test, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_category_count(df, categories):\n",
    "    sns.set(font_scale = 2)\n",
    "    plt.figure(figsize=(15,8))\n",
    "\n",
    "    ax= sns.barplot(categories, df.iloc[:,3:].sum().values)\n",
    "\n",
    "    plt.title(\"Jokes in each category\", fontsize=24)\n",
    "    plt.ylabel('Number of jokes', fontsize=18)\n",
    "    plt.xlabel('Joke Skill', fontsize=18)\n",
    "\n",
    "    #adding the text labels\n",
    "    rects = ax.patches\n",
    "    #print(rects)\n",
    "    labels = df.iloc[:,3:].sum().values\n",
    "    #print(labels)\n",
    "    for rect, label in zip(rects, labels):\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom', fontsize=18)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_category_count(df, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Calculating number of jokes having multiple labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_label(mlc_labels, multiLabel_counts):\n",
    "    sns.set(font_scale = 2)\n",
    "    plt.figure(figsize=(15,8))\n",
    "\n",
    "    ax = sns.barplot(mlc_labels, multiLabel_counts.values)\n",
    "\n",
    "    plt.title(\"Jokes having multiple labels \")\n",
    "    plt.ylabel('Number of jokes', fontsize=18)\n",
    "    plt.xlabel('Number of labels', fontsize=18)\n",
    "\n",
    "    #adding the text labels\n",
    "    rects = ax.patches\n",
    "    labels = multiLabel_counts.values\n",
    "    for rect, label in zip(rects, labels):\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_multiple_label(df):\n",
    "    rowSums = df.iloc[:,3:].sum(axis=1)\n",
    "    multiLabel_counts = rowSums.value_counts()\n",
    "    print(multiLabel_counts)\n",
    "    multiLabel_counts = multiLabel_counts.iloc[:]\n",
    "    #print(multiLabel_counts.index)\n",
    "    mlc_labels = ['L'+str(i) for i in multiLabel_counts.index]\n",
    "    print(mlc_labels)\n",
    "    \n",
    "    #plot_multiple_label(mlc_labels, multiLabel_counts)\n",
    "    ##return(mlc_labels, multiLabel_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    2964\n",
      "2     355\n",
      "3      41\n",
      "4       3\n",
      "0       2\n",
      "dtype: int64\n",
      "['L1', 'L2', 'L3', 'L4', 'L0']\n"
     ]
    }
   ],
   "source": [
    "print_multiple_label(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. WordCloud representation of most used words in each category of jokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import jieba\n",
    "import Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i, c in enumerate(categories):\\n    plt = MyWordCloud(plt, df, c, i+1)\\n\\n#plt.show()\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2880x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(40,25))\n",
    "\n",
    "def MyWordCloud(plt, df, field, position):\n",
    "    #subset = df[df.Pun==1]\n",
    "    subset = df.loc[df[field] == 1] # https://stackoverflow.com/questions/17071871/how-to-select-rows-from-a-dataframe-based-on-column-values\n",
    "    #print(subset.head()); exit\n",
    "    text = str(subset.Content.values)\n",
    "    words_list = jieba.lcut(Stopwords.clean_text(text))\n",
    "    text = Stopwords.clean_words(words_list)\n",
    "    cloud = WordCloud(\n",
    "                          #stopwords=STOPWORDS,\n",
    "                          stopwords=Stopwords.STOP_WORDS,\n",
    "                          background_color='black',\n",
    "                          font_path='SNsanafonGyou.ttf', # OSError: unknown file format\n",
    "                          collocations=False,\n",
    "                          width=2500,\n",
    "                          height=1800\n",
    "                         ).generate(\" \".join(text))\n",
    "\n",
    "    plt.subplot(3, 3, position)\n",
    "    plt.axis('off')\n",
    "    plt.title(field, fontsize=40)\n",
    "    plt.imshow(cloud)\n",
    "    return plt\n",
    "\n",
    "'''\n",
    "for i, c in enumerate(categories):\n",
    "    plt = MyWordCloud(plt, df, c, i+1)\n",
    "\n",
    "#plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in   all jokes: Max=2024, Min=10, Avg=134.07637444279345\n",
      "Number of characters in train jokes: Max=2024, Min=10, Avg=132.7906564163217\n",
      "Number of characters in  test jokes: Max=874, Min=12, Avg=135.3751493428913\n"
     ]
    }
   ],
   "source": [
    "# Compute statistics of the dataset: MaxLength, MinLength, AvgChars, AvgWords\n",
    "Len = df.Content.map(len)\n",
    "print(f'Number of characters in   all jokes: Max={max(Len)}, Min={min(Len)}, Avg={sum(Len)/len(Len)}')\n",
    "Len = train_random.Content.map(len)\n",
    "print(f'Number of characters in train jokes: Max={max(Len)}, Min={min(Len)}, Avg={sum(Len)/len(Len)}')\n",
    "Len = test.Content.map(len)\n",
    "print(f'Number of characters in  test jokes: Max={max(Len)}, Min={min(Len)}, Avg={sum(Len)/len(Len)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3365, 11)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set global variables: data\n",
    "data = df\n",
    "#data = df.loc[np.random.choice(df.index, size=3365)]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHtml(sentence):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "def cleanPunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Stopwords # import my own module with STOP_WORDS\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "    '''\n",
    "    Given a raw text string, return a clean text string.\n",
    "    Example: \n",
    "        input:  \"Years  passed. 多少   年过 去 了 。  \"\n",
    "        output: \"years passed.多少年过去了。\"\n",
    "    '''\n",
    "    text = str(text)\n",
    "    text = text.lower() # 'years  passed. 多少   年过 去 了 。'\n",
    "    # Next line will remove redundant white space for jeiba to cut\n",
    "    text = re.sub(r'\\s+([^a-zA-Z0-9.])', r'\\1', text) # years passed.多少年过去了。\n",
    "# see: https://stackoverflow.com/questions/16720541/python-string-replace-regular-expression\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "def clean_words(text, RmvStopWord=True, RmvMark=True):\n",
    "    words = jieba.lcut(text)\n",
    "#    print(\"After jieba.lcut():\", words)\n",
    "#    WL = [ w \n",
    "    WL = [ ps.stem(w)\n",
    "#    WL = [ wnl.lemmatize(w)\n",
    "        for w in words \n",
    "          if (not re.match(r'\\s', w)) # remove white spaces\n",
    "            and (RmvMark==False or not re.match(r'\\W', w)) # remove punctuations\n",
    "#            and (RmvMark==False or not re.match('^[a-z_]$', w)) # remove punctuations\n",
    "#            and (RmvMark==False or w not in PUNCTUATIONS)\n",
    "            and (RmvStopWord==False or w not in Stopwords.STOP_WORDS)\n",
    "            and (not re.match(r'^\\d+$', w)) # remove digit\n",
    "         ]\n",
    "    WL = \" \".join(WL)\n",
    "    return WL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/kg/jcdj05xn20144cv9kwywp26r0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID Title                                            Content  Pun  \\\n",
      "0  L0001  要求加薪  員工：老闆，您必須幫我加薪，已經有三家公司在找我了！     老闆：哪三家？     員工：...    1   \n",
      "1  L0002  查無此人  某市政府辦公大樓落成，門口缺副對聯。     副市長揮毫     上聯：說實話辦實事一身正氣...    0   \n",
      "2  L0003   遣散費  中午老闆視察自己的建築工地時，發現有個人在角落玩手機。     老闆：你月薪多少？     ...    0   \n",
      "3  L0004  職業習慣  一天，一位法官的女友看見兩個蚊子，便叫法官打死。     只見法官只把那個肚子飽飽的蚊子打死...    1   \n",
      "4  L0005  美女吵架  辦公室中兩位女同事吵起來了。     經理忍無可忍：「太不像話了！現在是什麼情況？你們把原因...    0   \n",
      "\n",
      "   Exaggeration  Anthropomorphism  Bridge_Inference  Illogical  Irony  \\\n",
      "0             0                 0                 0          0      0   \n",
      "1             0                 0                 0          0      1   \n",
      "2             0                 0                 0          0      0   \n",
      "3             0                 1                 0          0      0   \n",
      "4             0                 0                 1          0      0   \n",
      "\n",
      "   Imitation  Others  \n",
      "0          0       0  \n",
      "1          0       0  \n",
      "2          1       0  \n",
      "3          1       0  \n",
      "4          0       0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.606 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "data['Content'] = data['Content'].str.lower()\n",
    "#data['Content'] = data['Content'].apply(cleanHtml)\n",
    "#data['Content'] = data['Content'].apply(cleanPunc)\n",
    "#data['Content'] = data['Content'].apply(keepAlpha)\n",
    "data['Content'] = data['Content'].apply(clean_text)\n",
    "data['Content'] = data['Content'].apply(clean_words)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['zero','one','two','three','four','five','six','seven','eight','nine','ten','may','also','across','among','beside','however','yet','within'])\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)\n",
    "\n",
    "data['Content'] = data['Content'].apply(removeStopWords)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Pun</th>\n",
       "      <th>Exaggeration</th>\n",
       "      <th>Anthropomorphism</th>\n",
       "      <th>Bridge_Inference</th>\n",
       "      <th>Illogical</th>\n",
       "      <th>Irony</th>\n",
       "      <th>Imitation</th>\n",
       "      <th>Others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L0001</td>\n",
       "      <td>要求加薪</td>\n",
       "      <td>員工 老 闆 必須 幫 加薪 已經 三家 公司 找 老 闆 三家 員工 自來 水 公司 台電...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L0002</td>\n",
       "      <td>查無此人</td>\n",
       "      <td>某 市政府 辦公大樓 落成 門口 缺 副 對聯 副 市長 揮 毫上 聯 實話 辦實事 一身 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L0003</td>\n",
       "      <td>遣散費</td>\n",
       "      <td>中午 老 闆 視察 自己 建築 工地 時 發現 個 人 角落 玩手 機 老 闆 月薪 多少 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L0004</td>\n",
       "      <td>職業習慣</td>\n",
       "      <td>一天 一位 法官 女友 看見 兩個 蚊子 便 叫 法官 打死 只見 法官 只 那個 肚子 飽...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L0005</td>\n",
       "      <td>美女吵架</td>\n",
       "      <td>辦 公室 中 兩位 女同事 吵起 經理 忍無可忍 太不像 話 情況 原因 給我 清楚 兩人 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID Title                                            Content  Pun  \\\n",
       "0  L0001  要求加薪  員工 老 闆 必須 幫 加薪 已經 三家 公司 找 老 闆 三家 員工 自來 水 公司 台電...    1   \n",
       "1  L0002  查無此人  某 市政府 辦公大樓 落成 門口 缺 副 對聯 副 市長 揮 毫上 聯 實話 辦實事 一身 ...    0   \n",
       "2  L0003   遣散費  中午 老 闆 視察 自己 建築 工地 時 發現 個 人 角落 玩手 機 老 闆 月薪 多少 ...    0   \n",
       "3  L0004  職業習慣  一天 一位 法官 女友 看見 兩個 蚊子 便 叫 法官 打死 只見 法官 只 那個 肚子 飽...    1   \n",
       "4  L0005  美女吵架  辦 公室 中 兩位 女同事 吵起 經理 忍無可忍 太不像 話 情況 原因 給我 清楚 兩人 ...    0   \n",
       "\n",
       "   Exaggeration  Anthropomorphism  Bridge_Inference  Illogical  Irony  \\\n",
       "0             0                 0                 0          0      0   \n",
       "1             0                 0                 0          0      1   \n",
       "2             0                 0                 0          0      0   \n",
       "3             0                 1                 0          0      0   \n",
       "4             0                 0                 1          0      0   \n",
       "\n",
       "   Imitation  Others  \n",
       "0          0       0  \n",
       "1          0       0  \n",
       "2          1       0  \n",
       "3          1       0  \n",
       "4          0       0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "data['Content'] = data['Content'].apply(stemming)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1691, 11)\n",
      "(1674, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set global variables: train, test\n",
    "#train, test = train_test_split(data, random_state=42, test_size=0.30, shuffle=True)\n",
    "train, test = train_test_split(data, random_state=42, train_size=1691, shuffle=False)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables: train_text, test_text\n",
    "train_text = train['Content']\n",
    "test_text = test['Content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in   all jokes: Max=491, Min=3, Avg=41.6222882615156\n",
      "Number of words in train jokes: Max=491, Min=3, Avg=40.33234772324069\n",
      "Number of words in  test jokes: Max=290, Min=4, Avg=42.92532855436081\n"
     ]
    }
   ],
   "source": [
    "# Compute statistics of the dataset: MaxLength, MinLength, AvgChars, AvgWords\n",
    "Len = data.Content.map(lambda x: len(x.split()))\n",
    "print(f'Number of words in   all jokes: Max={max(Len)}, Min={min(Len)}, Avg={sum(Len)/len(Len)}')\n",
    "Len = train.Content.map(lambda x: len(x.split()))\n",
    "print(f'Number of words in train jokes: Max={max(Len)}, Min={min(Len)}, Avg={sum(Len)/len(Len)}')\n",
    "Len = test.Content.map(lambda x: len(x.split()))\n",
    "print(f'Number of words in  test jokes: Max={max(Len)}, Min={min(Len)}, Avg={sum(Len)/len(Len)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', \n",
    "                             ngram_range=(1,2), norm='l2')\n",
    "vectorizer.fit(train_text)\n",
    "vectorizer.fit(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set global variables: x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer.transform(train_text)\n",
    "y_train = train_random.drop(labels = ['ID', 'Title', 'Content'], axis=1)\n",
    "\n",
    "x_test = vectorizer.transform(test_text)\n",
    "y_test = test.drop(labels = ['ID', 'Title', 'Content'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain_tfidf.shape:(1691, 9559), xtest_tfidf.shape: (1674, 9559)\n",
      "xtrain_tfidf_ngram.shape:(1691, 9559), xtest_tfidf_ngram.shape: (1674, 9559)\n",
      "xtrain_tfidf_ngram_chars.shape:(1691, 9559), xtest_tfidf_ngram_chars.shape: (1674, 9559)\n",
      "It takes 2.45 seconds to convert 3 TFxIDF vectors.\n"
     ]
    }
   ],
   "source": [
    "time_TfidfVector = time.time()\n",
    "\n",
    "def Create_TFxIDF(data_text, train_text, test_text):\n",
    "\n",
    "# word level tf-idf\n",
    "    #tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=10000)\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "        stop_words=Stopwords.STOP_WORDS, max_df=0.95, min_df=2, max_features=10000)\n",
    "    tfidf_vect.fit(data_text)\n",
    "    xtrain_tfidf = tfidf_vect.transform(train_text)\n",
    "    xtest_tfidf = tfidf_vect.transform(test_text)\n",
    "    print(f\"xtrain_tfidf.shape:{xtrain_tfidf.shape}, xtest_tfidf.shape: {xtest_tfidf.shape}\")\n",
    "\n",
    "# word level ngram tf-idf \n",
    "    tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', \n",
    "                    stop_words=Stopwords.STOP_WORDS, max_df=0.95, min_df=2,\n",
    "                    ngram_range=(2,3), max_features=10000)\n",
    "    tfidf_vect_ngram.fit(data_text)\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_text)\n",
    "    xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_text)\n",
    "    print(f\"xtrain_tfidf_ngram.shape:{xtrain_tfidf.shape}, xtest_tfidf_ngram.shape: {xtest_tfidf.shape}\")\n",
    "\n",
    "# character level ngram tf-idf\n",
    "    tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', \n",
    "                    stop_words=Stopwords.STOP_WORDS, max_df=0.95, min_df=2,\n",
    "                    ngram_range=(2,3), max_features=10000)\n",
    "    tfidf_vect_ngram_chars.fit(data_text)\n",
    "    xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_text) \n",
    "    xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_text) \n",
    "    print(f\"xtrain_tfidf_ngram_chars.shape:{xtrain_tfidf.shape}, xtest_tfidf_ngram_chars.shape: {xtest_tfidf.shape}\")\n",
    "\n",
    "    print(\"It takes %4.2f seconds to convert 3 TFxIDF vectors.\"%(time.time()-time_TfidfVector))\n",
    "\n",
    "    return (xtrain_tfidf, xtest_tfidf, \n",
    "             xtrain_tfidf_ngram, xtest_tfidf_ngram,\n",
    "             xtrain_tfidf_ngram_chars, xtest_tfidf_ngram_chars,\n",
    "            tfidf_vect, tfidf_vect_ngram, tfidf_vect_ngram_chars)\n",
    "\n",
    "(xtrain_tfidf, xtest_tfidf, \n",
    " xtrain_tfidf_ngram, xtest_tfidf_ngram,\n",
    " xtrain_tfidf_ngram_chars, xtest_tfidf_ngram_chars,\n",
    " tfidf_vect, tfidf_vect_ngram, tfidf_vect_ngram_chars) = Create_TFxIDF(data.Content, train_text, test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1691, 9559) (1674, 9559)\n"
     ]
    }
   ],
   "source": [
    "# re-assign x_train and x_test to what we want\n",
    "x_train, x_test, vectorizer = xtrain_tfidf, xtest_tfidf, tfidf_vect\n",
    "#x_train, x_test, vectorizer = xtrain_tfidf_ngram, xtest_tfidf_ngram, tfidf_vect_ngram\n",
    "#x_train, x_test, vectorizer = xtrain_tfidf_ngram_chars, xtest_tfidf_ngram_chars, tfidf_vect_ngram_chars\n",
    "print(x_train.shape, x_test.shape)\n",
    "#print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Label Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Multiple Binary Classifications - (One Vs Rest Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the evaluation metrics\n",
    "def tcfunc(x, n=4): # trancate a number to have n decimal digits\n",
    "    d = '0' * n\n",
    "    d = int('1' + d)\n",
    "# https://stackoverflow.com/questions/4541155/check-if-a-number-is-int-or-float\n",
    "    if isinstance(x, (int, float)): return int(x * d) / d\n",
    "    return x\n",
    "\n",
    "def print_cls_report(y_true, prediction):\n",
    "    print('Test accuracy is %1.4f'%(accuracy_score(y_true, prediction)))\n",
    "\n",
    "    print(classification_report(y_true, prediction))\n",
    "    \n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
    "    print(\"\\tPrecision\\tRecall\\tF1\\tSupport\")\n",
    "    (Precision, Recall, F1, Support) = list(map(tcfunc, \n",
    "        precision_recall_fscore_support(y_true, prediction, average='micro')))\n",
    "    print(\"Micro\\t{}\\t{}\\t{}\\t{}\".format(Precision, Recall, F1, Support))\n",
    "\n",
    "    (Precision, Recall, F1, Support) = list(map(tcfunc, \n",
    "        precision_recall_fscore_support(y_true, prediction, average='macro')))\n",
    "    print(\"Macro\\t{}\\t{}\\t{}\\t{}\".format(Precision, Recall, F1, Support))\n",
    "    \n",
    "#    if True:\n",
    "    if False:\n",
    "        print(confusion_matrix(y_true, prediction))\n",
    "        try: \n",
    "            print(classification_report(y_true, prediction, digits=4))\n",
    "        except ValueError:\n",
    "            print('May be some category has no predicted samples')\n",
    "        show_confusion_matrix(prediction)\n",
    "\n",
    "\n",
    "    print(f'y_true.shape={y_true.shape}, prediction.shape={prediction.shape}')\n",
    "    #print(y_true.head())\n",
    "    #print(prediction[0:6])\n",
    "\n",
    "    # https://stackoverflow.com/questions/152580/whats-the-canonical-way-to-check-for-type-in-python\n",
    "    pred = prediction\n",
    "    if not isinstance(pred, np.ndarray): pred = prediction.toarray()\n",
    "    print(type(y_true), type(prediction), type(pred))\n",
    "    try:\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "        print('macro roc_auc_score is %1.4f'%(roc_auc_score(y_true, pred, average='macro'))) # default average=’macro’\n",
    "        print('micro roc_auc_score is %1.4f'%(roc_auc_score(y_true, pred, average='micro')))\n",
    "    except:\n",
    "        print(\"roc_auc_score error!!!\")\n",
    "    try:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, pred)\n",
    "        print(f'fpr={fpr}\\ntpr={tpr}\\nthresholds={thresholds}')\n",
    "    except:\n",
    "        print('roc_curve error!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Processing Pun jokes...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.6129\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.68      0.75      1413\n",
      "           1       0.13      0.26      0.18       261\n",
      "\n",
      "   micro avg       0.61      0.61      0.61      1674\n",
      "   macro avg       0.48      0.47      0.46      1674\n",
      "weighted avg       0.72      0.61      0.66      1674\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.6129\t0.6129\t0.6129\tNone\n",
      "Macro\t0.4821\t0.4708\t0.4613\tNone\n",
      "y_true.shape=(1674,), prediction.shape=(1674,)\n",
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.4708\n",
      "micro roc_auc_score is 0.4708\n",
      "fpr=[0.         0.32271762 1.        ]\n",
      "tpr=[0.         0.26436782 1.        ]\n",
      "thresholds=[2 1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Exaggeration jokes...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      1614\n",
      "           1       0.00      0.00      0.00        60\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      1674\n",
      "   macro avg       0.48      0.50      0.49      1674\n",
      "weighted avg       0.93      0.96      0.95      1674\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.9641\t0.9641\t0.9641\tNone\n",
      "Macro\t0.482\t0.5\t0.4908\tNone\n",
      "y_true.shape=(1674,), prediction.shape=(1674,)\n",
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5000\n",
      "micro roc_auc_score is 0.5000\n",
      "fpr=[0. 1.]\n",
      "tpr=[0. 1.]\n",
      "thresholds=[1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Anthropomorphism jokes...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1634\n",
      "           1       0.00      0.00      0.00        40\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1674\n",
      "   macro avg       0.49      0.50      0.49      1674\n",
      "weighted avg       0.95      0.98      0.96      1674\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.9761\t0.9761\t0.9761\tNone\n",
      "Macro\t0.488\t0.5\t0.4939\tNone\n",
      "y_true.shape=(1674,), prediction.shape=(1674,)\n",
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5000\n",
      "micro roc_auc_score is 0.5000\n",
      "fpr=[0. 1.]\n",
      "tpr=[0. 1.]\n",
      "thresholds=[1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Bridge_Inference jokes...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.8262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.90      1383\n",
      "           1       0.00      0.00      0.00       291\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1674\n",
      "   macro avg       0.41      0.50      0.45      1674\n",
      "weighted avg       0.68      0.83      0.75      1674\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.8261\t0.8261\t0.8261\tNone\n",
      "Macro\t0.413\t0.5\t0.4524\tNone\n",
      "y_true.shape=(1674,), prediction.shape=(1674,)\n",
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5000\n",
      "micro roc_auc_score is 0.5000\n",
      "fpr=[0. 1.]\n",
      "tpr=[0. 1.]\n",
      "thresholds=[1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Illogical jokes...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.7097\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83      1188\n",
      "           1       0.00      0.00      0.00       486\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1674\n",
      "   macro avg       0.35      0.50      0.42      1674\n",
      "weighted avg       0.50      0.71      0.59      1674\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.7096\t0.7096\t0.7096\tNone\n",
      "Macro\t0.3548\t0.5\t0.415\tNone\n",
      "y_true.shape=(1674,), prediction.shape=(1674,)\n",
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5000\n",
      "micro roc_auc_score is 0.5000\n",
      "fpr=[0. 1.]\n",
      "tpr=[0. 1.]\n",
      "thresholds=[1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Irony jokes...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1639\n",
      "           1       0.00      0.00      0.00        35\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1674\n",
      "   macro avg       0.49      0.50      0.49      1674\n",
      "weighted avg       0.96      0.98      0.97      1674\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.979\t0.979\t0.979\tNone\n",
      "Macro\t0.4895\t0.5\t0.4947\tNone\n",
      "y_true.shape=(1674,), prediction.shape=(1674,)\n",
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5000\n",
      "micro roc_auc_score is 0.5000\n",
      "fpr=[0. 1.]\n",
      "tpr=[0. 1.]\n",
      "thresholds=[1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Imitation jokes...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9343\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97      1564\n",
      "           1       0.00      0.00      0.00       110\n",
      "\n",
      "   micro avg       0.93      0.93      0.93      1674\n",
      "   macro avg       0.47      0.50      0.48      1674\n",
      "weighted avg       0.87      0.93      0.90      1674\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.9342\t0.9342\t0.9342\tNone\n",
      "Macro\t0.4671\t0.5\t0.483\tNone\n",
      "y_true.shape=(1674,), prediction.shape=(1674,)\n",
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5000\n",
      "micro roc_auc_score is 0.5000\n",
      "fpr=[0. 1.]\n",
      "tpr=[0. 1.]\n",
      "thresholds=[1 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Processing Others jokes...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.7145\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83      1196\n",
      "           1       0.00      0.00      0.00       478\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1674\n",
      "   macro avg       0.36      0.50      0.42      1674\n",
      "weighted avg       0.51      0.71      0.60      1674\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.7144\t0.7144\t0.7144\tNone\n",
      "Macro\t0.3572\t0.5\t0.4167\tNone\n",
      "y_true.shape=(1674,), prediction.shape=(1674,)\n",
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5000\n",
      "micro roc_auc_score is 0.5000\n",
      "fpr=[0. 1.]\n",
      "tpr=[0. 1.]\n",
      "thresholds=[1 0]\n",
      "CPU times: user 139 ms, sys: 91.3 ms, total: 230 ms\n",
      "Wall time: 2.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Using pipeline for applying logistic regression and one vs rest classifier\n",
    "LogReg_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),\n",
    "            ])\n",
    "\n",
    "#for category in categories:\n",
    "for category in categories:\n",
    "    printmd('**Processing {} jokes...**'.format(category))\n",
    "    \n",
    "    # Training logistic regression model on train data\n",
    "    LogReg_pipeline.fit(x_train, train_random[category])\n",
    "    \n",
    "    prediction = LogReg_pipeline.predict(x_test)\n",
    "    \n",
    "    print_cls_report(test[category], prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Multiple Binary Classifications - (Binary Relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use binary relevance, run \"pip install scikit-multilearn\" in advance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Next line refers to: http://scikit.ml/tutorial.html\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**BinaryRelevance with GaussianNB()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.0878\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.54      0.26       261\n",
      "           1       0.11      0.02      0.03        60\n",
      "           2       0.02      0.03      0.02        40\n",
      "           3       0.13      0.07      0.09       291\n",
      "           4       0.31      0.15      0.21       486\n",
      "           5       0.14      0.03      0.05        35\n",
      "           6       0.05      0.02      0.03       110\n",
      "           7       0.24      0.01      0.02       478\n",
      "\n",
      "   micro avg       0.18      0.14      0.16      1761\n",
      "   macro avg       0.15      0.11      0.09      1761\n",
      "weighted avg       0.21      0.14      0.12      1761\n",
      " samples avg       0.12      0.14      0.12      1761\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.182\t0.1402\t0.1584\tNone\n",
      "Macro\t0.1469\t0.1082\t0.0875\tNone\n",
      "y_true.shape=(1674, 8), prediction.shape=(1674, 8)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'scipy.sparse.csc.csc_matrix'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5038\n",
      "micro roc_auc_score is 0.5224\n",
      "roc_curve error!!!\n",
      "CPU times: user 11.6 s, sys: 786 ms, total: 12.4 s\n",
      "Wall time: 3.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initialize binary relevance multi-label classifier\n",
    "#   with a gaussian naive bayes base classifier\n",
    "classifier = BinaryRelevance(GaussianNB())\n",
    "\n",
    "# train\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "printmd('**BinaryRelevance with GaussianNB()**')\n",
    "print_cls_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**BinaryRelevance with LinearSVC()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.0884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.43      0.21       261\n",
      "           1       0.14      0.02      0.03        60\n",
      "           2       0.07      0.07      0.07        40\n",
      "           3       0.19      0.12      0.15       291\n",
      "           4       0.30      0.18      0.22       486\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.00      0.00      0.00       110\n",
      "           7       0.27      0.01      0.01       478\n",
      "\n",
      "   micro avg       0.18      0.14      0.15      1761\n",
      "   macro avg       0.14      0.10      0.09      1761\n",
      "weighted avg       0.21      0.14      0.12      1761\n",
      " samples avg       0.12      0.13      0.12      1761\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.1756\t0.1368\t0.1538\tNone\n",
      "Macro\t0.1391\t0.1032\t0.0871\tNone\n",
      "y_true.shape=(1674, 8), prediction.shape=(1674, 8)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'scipy.sparse.csc.csc_matrix'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5000\n",
      "micro roc_auc_score is 0.5198\n",
      "roc_curve error!!!\n",
      "CPU times: user 1.36 s, sys: 157 ms, total: 1.52 s\n",
      "Wall time: 648 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Next line refers to: http://scikit.ml/tutorial.html and \n",
    "#   http://scikit.ml/api/skmultilearn.problem_transform.br.html\n",
    "#classifier = BinaryRelevance(classifier=SVC(), require_dense=[False, True]) # 0.5 very bad!\n",
    "# https://scikit-learn.org/stable/modules/svm.html#unbalanced-problems\n",
    "classifier = BinaryRelevance(classifier=LinearSVC(class_weight='balanced')) # 0.5314\n",
    "#classifier = BinaryRelevance(classifier=LinearSVC()) # Test roc_auc_score is 0.5234\n",
    "\n",
    "# train\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "printmd('**BinaryRelevance with LinearSVC()**')\n",
    "print_cls_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Classifier Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using classifier chains\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Classifier Chains with LinearSVM()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.1535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.43      0.21       261\n",
      "           1       0.20      0.03      0.06        60\n",
      "           2       0.07      0.10      0.08        40\n",
      "           3       0.16      0.19      0.17       291\n",
      "           4       0.29      0.27      0.28       486\n",
      "           5       0.33      0.03      0.05        35\n",
      "           6       0.02      0.01      0.01       110\n",
      "           7       0.32      0.02      0.04       478\n",
      "\n",
      "   micro avg       0.18      0.18      0.18      1761\n",
      "   macro avg       0.19      0.13      0.11      1761\n",
      "weighted avg       0.23      0.18      0.15      1761\n",
      " samples avg       0.18      0.18      0.18      1761\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.1791\t0.1783\t0.1787\tNone\n",
      "Macro\t0.1915\t0.134\t0.1131\tNone\n",
      "y_true.shape=(1674, 8), prediction.shape=(1674, 8)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'scipy.sparse.csc.csc_matrix'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5009\n",
      "micro roc_auc_score is 0.5273\n",
      "roc_curve error!!!\n",
      "CPU times: user 2.99 s, sys: 916 ms, total: 3.91 s\n",
      "Wall time: 2.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initialize classifier chains multi-label classifier\n",
    "#classifier = ClassifierChain(LogisticRegression()) # Test roc_auc_score is 0.5159\n",
    "classifier = ClassifierChain(LinearSVC(class_weight='balanced')) #  0.5327\n",
    "\n",
    "# Training logistic regression model on train data\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "printmd('**Classifier Chains with LinearSVM()**')\n",
    "print_cls_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Label Powerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Label Powerset\n",
    "from skmultilearn.problem_transform import LabelPowerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Label Powerset with LinearSVM()**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.0753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.44      0.23       261\n",
      "           1       0.05      0.15      0.08        60\n",
      "           2       0.02      0.15      0.04        40\n",
      "           3       0.16      0.24      0.19       291\n",
      "           4       0.32      0.29      0.30       486\n",
      "           5       0.05      0.17      0.08        35\n",
      "           6       0.06      0.11      0.08       110\n",
      "           7       0.29      0.11      0.16       478\n",
      "\n",
      "   micro avg       0.16      0.23      0.19      1761\n",
      "   macro avg       0.14      0.21      0.15      1761\n",
      "weighted avg       0.23      0.23      0.20      1761\n",
      " samples avg       0.16      0.23      0.18      1761\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.1635\t0.2311\t0.1915\tNone\n",
      "Macro\t0.1408\t0.2061\t0.1454\tNone\n",
      "y_true.shape=(1674, 8), prediction.shape=(1674, 8)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'scipy.sparse.lil.lil_matrix'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5110\n",
      "micro roc_auc_score is 0.5261\n",
      "roc_curve error!!!\n",
      "CPU times: user 1.01 s, sys: 55.7 ms, total: 1.07 s\n",
      "Wall time: 348 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initialize label powerset multi-label classifier\n",
    "#classifier = LabelPowerset(LogisticRegression()) # Test roc_auc_score is 0.5059\n",
    "classifier = LabelPowerset(LinearSVC(class_weight='balanced')) # 0.5541\n",
    "# Test roc_auc_score is 0.5312 if xtrain_tfidf_ngram, xtest_tfidf_ngram are used.\n",
    "# Test roc_auc_score is 0.5474 if xtrain_tfidf_ngram_chars, xtest_tfidf_ngram_chars are used\n",
    "\n",
    "# train\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions = classifier.predict(x_test)\n",
    "\n",
    "printmd('**Label Powerset with LinearSVM()**')\n",
    "print_cls_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out/Skill_True.txt', 'w') as outF:\n",
    "    outF.write(y_test.to_csv(sep='\\t', index=False))\n",
    "\n",
    "# https://stackoverflow.com/questions/36967666/transform-scipy-sparse-csr-to-pandas\n",
    "with open('out/Skill_Pred.txt', 'w') as outF:\n",
    "    outF.write(pd.DataFrame(predictions.toarray(), columns=list(y_test.columns)).to_csv(sep='\\t', index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Adapted Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit.ml/api/api/skmultilearn.adapt.html#skmultilearn.adapt.MLkNN\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**MLkNN**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.0496\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.30      0.20       261\n",
      "           1       0.00      0.00      0.00        60\n",
      "           2       0.17      0.05      0.08        40\n",
      "           3       0.11      0.00      0.01       291\n",
      "           4       0.38      0.04      0.07       486\n",
      "           5       0.00      0.00      0.00        35\n",
      "           6       0.07      0.01      0.02       110\n",
      "           7       0.15      0.01      0.01       478\n",
      "\n",
      "   micro avg       0.17      0.06      0.09      1761\n",
      "   macro avg       0.13      0.05      0.05      1761\n",
      "weighted avg       0.19      0.06      0.06      1761\n",
      " samples avg       0.06      0.06      0.06      1761\n",
      "\n",
      "\tPrecision\tRecall\tF1\tSupport\n",
      "Micro\t0.1702\t0.0584\t0.087\tNone\n",
      "Macro\t0.1286\t0.0505\t0.0479\tNone\n",
      "y_true.shape=(1674, 8), prediction.shape=(1674, 8)\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'scipy.sparse.lil.lil_matrix'> <class 'numpy.ndarray'>\n",
      "macro roc_auc_score is 0.5028\n",
      "micro roc_auc_score is 0.5077\n",
      "roc_curve error!!!\n",
      "CPU times: user 1min 10s, sys: 290 ms, total: 1min 11s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "classifier_new = MLkNN(k=10)\n",
    "\n",
    "# Note that this classifier can throw up errors when handling sparse matrices.\n",
    "\n",
    "x_train = lil_matrix(x_train).toarray()\n",
    "y_train = lil_matrix(y_train).toarray()\n",
    "x_test = lil_matrix(x_test).toarray()\n",
    "\n",
    "# train\n",
    "classifier_new.fit(x_train, y_train)\n",
    "\n",
    "# predict\n",
    "predictions_new = classifier_new.predict(x_test)\n",
    "\n",
    "printmd('**MLkNN**')\n",
    "print_cls_report(y_test, predictions_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
